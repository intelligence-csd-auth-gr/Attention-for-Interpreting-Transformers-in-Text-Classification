{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfe2904",
   "metadata": {},
   "source": [
    "## Movies BERT Sentence\n",
    "In this notebook we examine the performance of interpretability techniques in the Movies dataset using BERT on sentence level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, average_precision_score\n",
    "from dataset import Dataset\n",
    "from myModel import MyModel, MyDataset\n",
    "from myExplainers import MyExplainer\n",
    "from myEvaluation import MyEvaluation\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import csv\n",
    "import warnings\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from scipy.special import softmax\n",
    "from helper import print_results, print_results_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabb59f",
   "metadata": {},
   "source": [
    "Loading model and dataset, defining transformer model, and if rationales are available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fbe8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "model_path = 'Trained Models/'\n",
    "save_path = '/home/myloniko/ethos/Results/MV/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbd028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert'\n",
    "existing_rationales = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f07c5",
   "metadata": {},
   "source": [
    "Load MyModel, and the subsequent tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfdbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'single_label'\n",
    "sentence_level = True\n",
    "labels = 2\n",
    "model = MyModel(model_path, 'bert_movies', model_name, task, labels, False)\n",
    "max_sequence_len = model.tokenizer.max_len_single_sentence\n",
    "tokenizer = model.tokenizer\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "model.trainer.model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6b271",
   "metadata": {},
   "source": [
    "Loading dataset and the rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45aaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = Dataset(path = data_path)\n",
    "x, y, label_names, rationales = mv.load_movies(level='sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52ce09",
   "metadata": {},
   "source": [
    "Splitting dataset to train/val/test sets (70/10/20%), we also remove the texts whose size exceed BERT's size limit, specifically 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72adc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rationales = True\n",
    "indices = np.arange(len(y))\n",
    "train_texts, test_texts, train_labels, test_labels, _, test_indexes = train_test_split(x, list(y), indices, test_size=.2, random_state=42)\n",
    "if existing_rationales:\n",
    "    test_rationales = [rationales[x] for x in test_indexes]\n",
    "size = (0.1 * len(y)) / len(train_labels)\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(list(train_texts), train_labels, test_size=size, random_state=42)\n",
    "train_texts.append(test_texts[84])\n",
    "train_labels.append(test_labels[84])\n",
    "train_texts.append(test_texts[72])\n",
    "train_labels.append(test_labels[72])\n",
    "test_texts.pop(84)\n",
    "test_labels.pop(84)\n",
    "test_rationales.pop(84)\n",
    "test_texts.pop(72)\n",
    "test_labels.pop(72)\n",
    "test_rationales.pop(72)\n",
    "test_texts.pop(63)\n",
    "test_labels.pop(63)\n",
    "test_rationales.pop(63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76554a",
   "metadata": {},
   "source": [
    "Preparing the rationales (we put 0 in the class that has no rationales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "282d025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test_rationales = []\n",
    "for i in range(len(test_rationales)):\n",
    "    if (test_labels[i] == 1):\n",
    "        test_test_rationales.append([[0]*len(test_rationales[i][:-1]),list(test_rationales[i][:-1])])\n",
    "    else:\n",
    "        test_test_rationales.append([list(test_rationales[i][:-1]),[0]* len(test_rationales[i][:-1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2aa3d6",
   "metadata": {},
   "source": [
    "Then, we measure the performance of the model using average precision score and f1 score (both macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eaae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for test_text in test_texts:\n",
    "    outputs = model.my_predict(test_text)\n",
    "    predictions.append(outputs[0])\n",
    "\n",
    "pred_labels = []\n",
    "for prediction in predictions:\n",
    "    pred_labels.append(np.argmax(softmax(prediction)))\n",
    "\n",
    "def average_precision_wrapper(y, y_pred, view):\n",
    "    return average_precision_score(y, y_pred.toarray(), average=view)\n",
    "\n",
    "average_precision_score(test_labels, pred_labels, average='macro'), accuracy_score(test_labels, pred_labels), f1_score(test_labels, pred_labels, average='macro'), f1_score(test_labels, pred_labels, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484a4bc",
   "metadata": {},
   "source": [
    "We initialize the explainers and the evaluation module, as well as we define the metrics we want to use. In this case, we use F=Faithfulness, FTP=RFT (Ranked Faithful Truthfulness), NZW=Complexity, AUPRC=For the rationales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b6e6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_explainers = MyExplainer(label_names, model, True, 'â€¡')\n",
    "\n",
    "my_evaluators = MyEvaluation(label_names, model.my_predict, True, True)\n",
    "my_evaluatorsP = MyEvaluation(label_names, model.my_predict, True, False)\n",
    "evaluation =  {'F':my_evaluators.faithfulness, 'FTP': my_evaluators.faithful_truthfulness_penalty, \n",
    "          'NZW': my_evaluators.nzw, 'AUPRC': my_evaluators.auprc}\n",
    "evaluationP = {'F':my_evaluatorsP.faithfulness, 'FTP': my_evaluatorsP.faithful_truthfulness_penalty, \n",
    "          'NZW': my_evaluatorsP.nzw, 'AUPRC': my_evaluators.auprc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66cf43",
   "metadata": {},
   "source": [
    "We start the experiment measuring the performance of LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f62ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    file_name = save_path + 'MOVIES_BERT_SENTENCE_LIME_IG_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'F':[], 'FTP':[], 'NZW':[], 'AUPRC':[]}\n",
    "    metricsP = {'F':[], 'FTP':[], 'NZW':[], 'AUPRC':[]}\n",
    "    time_r = [[],[]]\n",
    "    my_explainers.neighbours = 200\n",
    "    techniques = [my_explainers.lime, my_explainers.ig] \n",
    "    for ind in tqdm(range(0,len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        prediction, _, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "    \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for technique in techniques:\n",
    "            ts = time.time()\n",
    "            temp = technique(instance, prediction, tokens, mask, _, _)\n",
    "            temp_tokens = tokens.copy()\n",
    "            if sentence_level:\n",
    "                temp_tokens = temp[0].copy()[0]\n",
    "                temp = temp[1].copy()\n",
    "            interpretations.append([np.array(i)/np.max(abs(np.array(i))) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            for interpretation in interpretations:\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_rational))\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        my_evaluators.clear_states()\n",
    "        for metric in metrics.keys():\n",
    "            evaluatedP = []\n",
    "            for interpretation in interpretations:\n",
    "                evaluatedP.append(evaluationP[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_rational))\n",
    "            metricsP[metric].append(evaluatedP)\n",
    "        with open(file_name+'(A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'(P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d0061",
   "metadata": {},
   "source": [
    "We present the results for LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94780ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+'(A)', [' LIME', ' IG  '], metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+'(P)', [' LIME', ' IG  '], metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d5082a",
   "metadata": {},
   "source": [
    "Then, we perform the experiments for the different attention setups!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean', 'Multi'] + list(range(12)):\n",
    "    for ce in ['Mean'] + list(range(12)):\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'MV_BERT_ATTENTION_SENTENCE'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    metricsP = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, attention, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for con in conf:\n",
    "            ts = time.time()\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, _)\n",
    "            temp_tokens = tokens.copy()\n",
    "            if sentence_level:\n",
    "                temp_tokens = temp[0].copy()[0]\n",
    "                temp = temp[1].copy()\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b.append(k)\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluationP[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b2.append(k)\n",
    "            metricsP[metric].append(evaluated)\n",
    "        with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(),time_r.mean(axis=1).max(), time_r.mean(axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb54351",
   "metadata": {},
   "source": [
    "We present the results of the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98523ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e0590",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation script at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4cf86",
   "metadata": {},
   "source": [
    "We repeat the process with Attention Scores with negative values (A*), thus by skipping the Softmax function. In the attention setups, we exclude the multiplication option in heads and layers, as a few combinations reach +/-inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean'] + list(range(12)):\n",
    "    for ce in ['Mean'] + list(range(12)):\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e154cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'MV_BERT_A_ATTENTION_NO_SOFTMAX_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    metricsP = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, _, hidden_states = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        \n",
    "        attention = []\n",
    "        for la in range(12):\n",
    "            our_new_layer = []\n",
    "            bob = model.trainer.model.base_model.encoder.layer[la].attention\n",
    "            has = hidden_states[la]\n",
    "            aaa = bob.self.key(torch.tensor(has).to('cuda'))\n",
    "            bbb = bob.self.query(torch.tensor(has).to('cuda'))\n",
    "            for he in range(12):\n",
    "                attention_scores = torch.matmul(bbb[:,he*64:(he+1)*64], aaa[:,he*64:(he+1)*64].transpose(-1, -2))\n",
    "                attention_scores = attention_scores / math.sqrt(64)\n",
    "                our_new_layer.append(attention_scores.cpu().detach().numpy())\n",
    "            attention.append(our_new_layer)\n",
    "        attention = np.array(attention)\n",
    "        \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for con in conf:\n",
    "            ts = time.time()\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, _)\n",
    "            temp_tokens = tokens.copy()\n",
    "            if sentence_level:\n",
    "                temp_tokens = temp[0].copy()[0]\n",
    "                temp = temp[1].copy()\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b.append(k)\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluationP[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b2.append(k)\n",
    "            metricsP[metric].append(evaluated)        \n",
    "        with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(),time_r.mean(axis=1).max(), time_r.mean(axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6611c",
   "metadata": {},
   "source": [
    "We present the results for the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa164be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb78dd",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation script at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13bbe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269c305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
