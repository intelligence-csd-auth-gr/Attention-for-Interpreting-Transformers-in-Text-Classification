{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c64a723",
   "metadata": {},
   "source": [
    "## Movies DistilBERT\n",
    "In this notebook we examine the performance of interpretability techniques in the Movies dataset using DistilBERT on token level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5383db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, average_precision_score\n",
    "from dataset import Dataset\n",
    "from myModel import MyModel, MyDataset\n",
    "from myExplainers import MyExplainer\n",
    "from myEvaluation import MyEvaluation\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import csv\n",
    "import warnings\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from scipy.special import softmax\n",
    "from helper import print_results, print_results_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd23c10",
   "metadata": {},
   "source": [
    "Loading model and dataset, defining transformer model, and if rationales are available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15224761",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "model_path = 'Trained Models/'\n",
    "save_path = '/home/myloniko/ethos/Results/MV/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c3c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert'\n",
    "existing_rationales = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb6bbc",
   "metadata": {},
   "source": [
    "Load MyModel, and the subsequent tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'single_label'\n",
    "labels = 2\n",
    "model = MyModel(model_path, 'distilbert_movies', model_name, task, labels, False)\n",
    "max_sequence_len = model.tokenizer.max_len_single_sentence\n",
    "tokenizer = model.tokenizer\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "model.trainer.model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a69df5",
   "metadata": {},
   "source": [
    "Loading dataset and the rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = Dataset(path = data_path)\n",
    "x, y, label_names, rationales = mv.load_movies(level='token')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd42b6",
   "metadata": {},
   "source": [
    "Splitting dataset to train/val/test sets (70/10/20%), we also remove the texts whose size exceed BERT's size limit, specifically 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rationales = True\n",
    "indices = np.arange(len(y))\n",
    "train_texts, test_texts, train_labels, test_labels, _, test_indexes = train_test_split(x, list(y), indices, test_size=.2, random_state=42)\n",
    "if existing_rationales:\n",
    "    test_rationales = [rationales[x] for x in test_indexes]\n",
    "size = (0.1 * len(y)) / len(train_labels)\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(list(train_texts), train_labels, test_size=size, random_state=42)\n",
    "train_texts.append(test_texts[84])\n",
    "train_labels.append(test_labels[84])\n",
    "train_texts.append(test_texts[72])\n",
    "train_labels.append(test_labels[72])\n",
    "test_texts.pop(84)\n",
    "test_labels.pop(84)\n",
    "test_rationales.pop(84)\n",
    "test_texts.pop(72)\n",
    "test_labels.pop(72)\n",
    "test_rationales.pop(72)\n",
    "test_texts.pop(63)\n",
    "test_labels.pop(63)\n",
    "test_rationales.pop(63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838e8ff",
   "metadata": {},
   "source": [
    "Preparing the rationales (we put 0 in the class that has no rationales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c4653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test_rationales = []\n",
    "for i in range(len(test_rationales)):\n",
    "    if (test_labels[i] == 1):\n",
    "        test_test_rationales.append([[0]*len(test_rationales[i]),test_rationales[i]])\n",
    "    else:\n",
    "        test_test_rationales.append([test_rationales[i],[0]* len(test_rationales[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2cb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rationale= []\n",
    "for i in range(82):\n",
    "    rationale = []\n",
    "    test_t = test_texts[i].split(' ')\n",
    "    for j in range(2):\n",
    "        label_rational = []\n",
    "        for k in range(len(test_t)):\n",
    "            for r in tokenizer.tokenize(test_t[k]):\n",
    "                #if r == '.':\n",
    "                #    print(r)\n",
    "                #print(r)\n",
    "                rationall = 1 if test_test_rationales[i][j][k] > 0 else 0\n",
    "                label_rational.append(rationall)\n",
    "        rationale.append(label_rational)\n",
    "    new_rationale.append(rationale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b4a7e",
   "metadata": {},
   "source": [
    "Then, we measure the performance of the model using average precision score and f1 score (both macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3119908",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for test_text in test_texts:\n",
    "    outputs = model.my_predict(test_text)\n",
    "    predictions.append(outputs[0])\n",
    "\n",
    "pred_labels = []\n",
    "for prediction in predictions:\n",
    "    pred_labels.append(np.argmax(softmax(prediction)))\n",
    "\n",
    "def average_precision_wrapper(y, y_pred, view):\n",
    "    return average_precision_score(y, y_pred.toarray(), average=view)\n",
    "\n",
    "average_precision_score(test_labels, pred_labels, average='macro'), accuracy_score(test_labels, pred_labels), f1_score(test_labels, pred_labels, average='macro'), f1_score(test_labels, pred_labels, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c31ad",
   "metadata": {},
   "source": [
    "We initialize the explainers and the evaluation module, as well as we define the metrics we want to use. In this case, we use F=Faithfulness, FTP=RFT (Ranked Faithful Truthfulness), NZW=Complexity, AUPRC=For the rationales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "479cac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_explainers = MyExplainer(label_names, model)\n",
    "\n",
    "my_evaluators = MyEvaluation(label_names, model.my_predict, False, True)\n",
    "my_evaluatorsP = MyEvaluation(label_names, model.my_predict, False, False)\n",
    "evaluation =  {'F':my_evaluators.faithfulness, 'FTP': my_evaluators.faithful_truthfulness_penalty, \n",
    "          'NZW': my_evaluators.nzw, 'AUPRC': my_evaluators.auprc}\n",
    "evaluationP = {'F':my_evaluatorsP.faithfulness, 'FTP': my_evaluatorsP.faithful_truthfulness_penalty, \n",
    "          'NZW': my_evaluatorsP.nzw, 'AUPRC': my_evaluators.auprc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c2a677",
   "metadata": {},
   "source": [
    "We start the experiment measuring the performance of LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7683811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    file_name = save_path + 'MOVIES_DISTILBERT_LIME_IG_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'F':[], 'FTP':[], 'NZW':[], 'AUPRC':[]}\n",
    "    metricsP = {'F':[], 'FTP':[], 'NZW':[], 'AUPRC':[]}\n",
    "    time_r = [[],[]]\n",
    "    my_explainers.neighbours = 200\n",
    "    techniques = [my_explainers.lime, my_explainers.ig] \n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = new_rationale[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        prediction, _, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "    \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for technique in techniques:\n",
    "            ts = time.time()\n",
    "            temp = technique(instance, prediction, tokens, mask, _, _)\n",
    "            interpretations.append([np.array(i)/np.max(abs(np.array(i))) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            for interpretation in interpretations:\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        my_evaluators.clear_states()\n",
    "        for metric in metrics.keys():\n",
    "            evaluatedP = []\n",
    "            for interpretation in interpretations:\n",
    "                evaluatedP.append(evaluationP[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "            metricsP[metric].append(evaluatedP)\n",
    "        with open(file_name+'(A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'(P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea29a8",
   "metadata": {},
   "source": [
    "We present the results for LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba97ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+'(A)', [' LIME', ' IG  '], metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93201b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+'(P)', [' LIME', ' IG  '], metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528bf6e",
   "metadata": {},
   "source": [
    "Then, we perform the experiments for the different attention setups!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean', 'Multi'] + list(range(6)):\n",
    "    for ce in ['Mean'] + list(range(12)):\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'MV_DISTILBERT_ATTENTION_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    metricsP = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = new_rationale[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, attention, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for con in conf:\n",
    "            ts = time.time()\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, _)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b.append(k)\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluationP[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b2.append(k)\n",
    "            metricsP[metric].append(evaluated)\n",
    "        with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(),time_r.mean(axis=1).max(), time_r.mean(axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00a43e",
   "metadata": {},
   "source": [
    "We present the results of the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07053ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01174a9b",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation script at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142cec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185831a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa075422",
   "metadata": {},
   "source": [
    "We repeat the process with Attention Scores with negative values (A*), thus by skipping the Softmax function. In the attention setups, we exclude the multiplication option in heads and layers, as a few combinations reach +/-inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean'] + list(range(6)):\n",
    "    for ce in ['Mean'] + list(range(12)):\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'MV_DISTILBERT_A_ATTENTION_NO_SOFTMAX_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    metricsP = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = new_rationale[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, _, hidden_states = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        \n",
    "        attention = []\n",
    "        for la in range(6):\n",
    "            our_new_layer = []\n",
    "            bob =  model.trainer.model.base_model.transformer.layer[la].attention\n",
    "            has = hidden_states[la]\n",
    "            aaa = bob.k_lin(torch.tensor(has).to('cuda'))\n",
    "            bbb = bob.q_lin(torch.tensor(has).to('cuda'))\n",
    "            for he in range(12):\n",
    "                bbb = bbb / math.sqrt(64)\n",
    "                attention_scores = torch.matmul(bbb[:,he*64:(he+1)*64], aaa[:,he*64:(he+1)*64].transpose(-1, -2))\n",
    "                our_new_layer.append(attention_scores.cpu().detach().numpy())\n",
    "            attention.append(our_new_layer)\n",
    "        attention = np.array(attention)\n",
    "        interpretations = []\n",
    "        \n",
    "        kk = 0\n",
    "        for con in conf:\n",
    "            ts = time.time()\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, _)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b.append(k)\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluationP[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b2.append(k)\n",
    "            metricsP[metric].append(evaluated)        \n",
    "        with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(),time_r.mean(axis=1).max(), time_r.mean(axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bcd2b",
   "metadata": {},
   "source": [
    "We present the results for the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7facf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a231692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1533c2d",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation script at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010ad8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
