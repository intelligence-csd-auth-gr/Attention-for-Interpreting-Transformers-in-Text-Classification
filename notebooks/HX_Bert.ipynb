{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4fd936",
   "metadata": {},
   "source": [
    "## HX DistilBERT\n",
    "In this notebook we examine the performance of interpretability techniques in the HX dataset using BERT on token level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8051e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, average_precision_score\n",
    "from dataset import Dataset\n",
    "from myModel import MyModel, MyDataset\n",
    "from myExplainers import MyExplainer\n",
    "from myEvaluation import MyEvaluation\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import csv\n",
    "import warnings\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from scipy.special import softmax\n",
    "from helper import print_results, print_results_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a1a2fa",
   "metadata": {},
   "source": [
    "Loading model and dataset, defining transformer model, and if rationales are available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed200d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "model_path = 'Trained Models/'\n",
    "save_path = '/home/myloniko/ethos/Results/HX/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea29631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert'\n",
    "existing_rationales = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91841153",
   "metadata": {},
   "source": [
    "Load MyModel, and the subsequent tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'single_label'\n",
    "labels = 2\n",
    "model = MyModel(model_path, 'bert_hx', model_name, task, labels, False)\n",
    "max_sequence_len = model.tokenizer.max_len_single_sentence\n",
    "tokenizer = model.tokenizer\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "model.trainer.model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3c9f2",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a857ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hx = Dataset(path = data_path)\n",
    "x, y, label_names, rationales = hx.load_hatexplain(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8940d69",
   "metadata": {},
   "source": [
    "Splitting dataset to train/val/test sets (70/10/20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd257db",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(y))\n",
    "train_texts, test_texts, train_labels, test_labels, _, test_indexes = train_test_split(x, y,  indices, stratify=y, test_size=.2, random_state=42)\n",
    "if existing_rationales:\n",
    "    test_rationales = [rationales[x] for x in test_indexes]\n",
    "\n",
    "size = (0.1 * len(y)) / len(train_labels)\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(list(train_texts), train_labels, stratify=train_labels, test_size=size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf0301",
   "metadata": {},
   "source": [
    "Preparing the rationales. HX contains rationales only for the hate speech class. Thus, we add a zero value as the rationales of the non hate speech class, and the evaluation metric will handle it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d3f0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test_rationales = []\n",
    "for test_rational in test_rationales:\n",
    "    test_test_rationales.append([0,test_rational])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91feb7e1",
   "metadata": {},
   "source": [
    "Then, we measure the performance of the model using average precision score and f1 score (both macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa84ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for test_text in test_texts:\n",
    "    outputs = model.my_predict(test_text)\n",
    "    predictions.append(outputs[0])\n",
    "\n",
    "pred_labels = []\n",
    "for prediction in predictions:\n",
    "    pred_labels.append(np.argmax(softmax(prediction)))\n",
    "\n",
    "def average_precision_wrapper(y, y_pred, view):\n",
    "    return average_precision_score(y, y_pred.toarray(), average=view)\n",
    "\n",
    "average_precision_score(test_labels, pred_labels, average='macro'), accuracy_score(test_labels, pred_labels), f1_score(test_labels, pred_labels, average='macro'), f1_score(test_labels, pred_labels, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792a850",
   "metadata": {},
   "source": [
    "We initialize the explainers and the evaluation module, as well as we define the metrics we want to use. In this case, we use F=Faithfulness, FTP=RFT (Ranked Faithful Truthfulness), NZW=Complexity, AUPRC=For the rationales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06170276",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_explainers = MyExplainer(label_names, model)\n",
    "\n",
    "my_evaluators = MyEvaluation(label_names, model.my_predict, False, True)\n",
    "my_evaluatorsP = MyEvaluation(label_names, model.my_predict, False, False)\n",
    "evaluation =  {'F':my_evaluators.faithfulness, 'FTP': my_evaluators.faithful_truthfulness_penalty, \n",
    "          'NZW': my_evaluators.nzw, 'AUPRC': my_evaluators.auprc}\n",
    "evaluationP = {'F':my_evaluatorsP.faithfulness, 'FTP': my_evaluatorsP.faithful_truthfulness_penalty, \n",
    "          'NZW': my_evaluatorsP.nzw, 'AUPRC': my_evaluators.auprc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42fce6",
   "metadata": {},
   "source": [
    "We start the experiment measuring the performance of LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    file_name = save_path + 'HX_BERT_LIME_IG_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'F':[], 'FTP':[], 'NZW':[], 'AUPRC':[]}\n",
    "    metricsP = {'F':[], 'FTP':[], 'NZW':[], 'AUPRC':[]}\n",
    "    time_r = [[],[]]\n",
    "    my_explainers.neighbours = 2000\n",
    "    techniques = [my_explainers.lime, my_explainers.ig] \n",
    "    for ind in tqdm(range(0,len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        prediction, _, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "    \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for technique in techniques:\n",
    "            ts = time.time()\n",
    "            temp = technique(instance, prediction, tokens, mask, _, _)\n",
    "            interpretations.append([np.array(i)/np.max(abs(np.array(i))) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            for interpretation in interpretations:\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        my_evaluators.clear_states()\n",
    "        for metric in metrics.keys():\n",
    "            evaluatedP = []\n",
    "            for interpretation in interpretations:\n",
    "                evaluatedP.append(evaluationP[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "            metricsP[metric].append(evaluatedP)\n",
    "        with open(file_name+'(A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'(P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a5ca8",
   "metadata": {},
   "source": [
    "We present the results for LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+'(A)', [' LIME', ' IG  '], metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+'(P)', [' LIME', ' IG  '], metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a0105",
   "metadata": {},
   "source": [
    "Then, we perform the experiments for the different attention setups!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be0901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean', 'Multi'] + list(range(12)):\n",
    "    for ce in ['Mean', 'Multi'] + list(range(12)):\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5800f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'HX_BERT_ATTENTION_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    metricsP = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, attention, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for con in conf:\n",
    "            ts = time.time()\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, _)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b.append(k)\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluationP[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b2.append(k)\n",
    "            metricsP[metric].append(evaluated)\n",
    "        with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(),time_r.mean(axis=1).max(), time_r.mean(axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a928604",
   "metadata": {},
   "source": [
    "We present the results of the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb15d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ae670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae2fe9",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation script at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d238e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1c390",
   "metadata": {},
   "source": [
    "We repeat the process with Attention Scores with negative values (A*), thus by skipping the Softmax function. In the attention setups, we exclude the multiplication option in heads and layers, as a few combinations reach +/-inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean', 'Multi'] + list(range(12)):\n",
    "    for ce in ['Mean'] + list(range(12)):\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc55784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'HX_BERT_A_ATTENTION_NO_SOFTMAX_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    metricsP = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, _, hidden_states = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        \n",
    "        attention = []\n",
    "        for la in range(12):\n",
    "            our_new_layer = []\n",
    "            bob = model.trainer.model.base_model.encoder.layer[la].attention\n",
    "            has = hidden_states[la]\n",
    "            aaa = bob.self.key(torch.tensor(has).to('cuda'))\n",
    "            bbb = bob.self.query(torch.tensor(has).to('cuda'))\n",
    "            for he in range(12):\n",
    "                attention_scores = torch.matmul(bbb[:,he*64:(he+1)*64], aaa[:,he*64:(he+1)*64].transpose(-1, -2))\n",
    "                attention_scores = attention_scores / math.sqrt(64)\n",
    "                our_new_layer.append(attention_scores.cpu().detach().numpy())\n",
    "            attention.append(our_new_layer)\n",
    "        attention = np.array(attention)\n",
    "        \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for con in conf:\n",
    "            ts = time.time()\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, _)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b.append(k)\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluationP[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b2.append(k)\n",
    "            metricsP[metric].append(evaluated)        \n",
    "        with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(),time_r.mean(axis=1).max(), time_r.mean(axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2215f1",
   "metadata": {},
   "source": [
    "We present the results for the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18029fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3fefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57faaec4",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation script at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fce5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90081b0",
   "metadata": {},
   "source": [
    "# Time response experiment\n",
    "In this part of the notebook, we present some time-response experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e4188",
   "metadata": {},
   "source": [
    "Calculate the time response for LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9199348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    time_r = [[],[]]\n",
    "    my_explainers.neighbours = 2000\n",
    "    techniques = [my_explainers.lime, my_explainers.ig] \n",
    "    for ind in tqdm(range(0,100)):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        prediction, _, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "    \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for technique in techniques:\n",
    "            ts = time.time()\n",
    "            temp = technique(instance, prediction, tokens, mask, _, _)\n",
    "            interpretations.append([np.array(i)/np.max(abs(np.array(i))) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78949b",
   "metadata": {},
   "source": [
    "Calculate the time response for Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01761352",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean', 'Multi'] + list(range(12)):\n",
    "    for ce in ['Mean'] + list(range(12)):\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "import time \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    metrics = {'FTP':[]}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(0,100)):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, attention, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        \n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for con in conf:\n",
    "            ts = time.time()\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, _)\n",
    "            interpretations.append([i for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b.append(k)\n",
    "time_r = np.array(time_r)\n",
    "time_b = np.array(time_b)\n",
    "time_r.mean(axis=1).min(),time_r.mean(axis=1).max(), time_r.mean(axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
