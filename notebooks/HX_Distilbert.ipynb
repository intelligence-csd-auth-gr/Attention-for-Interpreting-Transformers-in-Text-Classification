{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d206f4",
   "metadata": {},
   "source": [
    "## HX DistilBERT\n",
    "In this notebook we examine the performance of interpretability techniques in the HX dataset using DistilBERT on token level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, average_precision_score\n",
    "from dataset import Dataset\n",
    "from myModel import MyModel, MyDataset\n",
    "from myExplainers import MyExplainer\n",
    "from myEvaluation import MyEvaluation\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import csv\n",
    "import warnings\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from scipy.special import softmax\n",
    "from helper import print_results, print_results_ap\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc01ad",
   "metadata": {},
   "source": [
    "Loading model and dataset, defining transformer model, and if rationales are available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bd45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "model_path = 'Trained Models/'\n",
    "save_path = '/home/myloniko/ethos/Results/HX/'\n",
    "model_name = 'distilbert'\n",
    "existing_rationales = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8533969",
   "metadata": {},
   "source": [
    "Load MyModel, and the subsequent tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df62af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'single_label'\n",
    "labels = 2\n",
    "model = MyModel(model_path, 'distilbert_hx', model_name, task, labels, False)\n",
    "max_sequence_len = model.tokenizer.max_len_single_sentence\n",
    "tokenizer = model.tokenizer\n",
    "torch.cuda.is_available()\n",
    "model.trainer.model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534146c",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b262a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hx = Dataset(path=data_path)\n",
    "x, y, label_names, rationales = hx.load_hatexplain(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c44eae",
   "metadata": {},
   "source": [
    "Splitting dataset to train/val/test sets (70/10/20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ef713",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(y))\n",
    "train_texts, test_texts, train_labels, test_labels, _, test_indexes = train_test_split(\n",
    "    x, y,  indices, stratify=y, test_size=.2, random_state=42)\n",
    "if existing_rationales:\n",
    "    test_rationales = [rationales[x] for x in test_indexes]\n",
    "\n",
    "size = (0.1 * len(y)) / len(train_labels)\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(\n",
    "    list(train_texts), train_labels, stratify=train_labels, test_size=size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9c9ca",
   "metadata": {},
   "source": [
    "Preparing the rationales. HX contains rationales only for the hate speech class. Thus, we add a zero value as the rationales of the non hate speech class, and the evaluation metric will handle it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef153395",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test_rationales = []\n",
    "for test_rational in test_rationales:\n",
    "    test_test_rationales.append([0, test_rational])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918328f0",
   "metadata": {},
   "source": [
    "Then, we measure the performance of the model using average precision score and f1 score (both macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for test_text in test_texts:\n",
    "    outputs = model.my_predict(test_text)\n",
    "    predictions.append(outputs[0])\n",
    "\n",
    "pred_labels = []\n",
    "for prediction in predictions:\n",
    "    pred_labels.append(np.argmax(softmax(prediction)))\n",
    "\n",
    "def average_precision_wrapper(y, y_pred, view):\n",
    "    return average_precision_score(y, y_pred.toarray(), average=view)\n",
    "\n",
    "average_precision_score(test_labels, pred_labels, average='macro'), f1_score(test_labels, pred_labels, average='macro'),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f9919",
   "metadata": {},
   "source": [
    "We initialize the explainers and the evaluation module, as well as we define the metrics we want to use. In this case, we use F=Faithfulness, FTP=RFT (Ranked Faithful Truthfulness), NZW=Complexity, AUPRC=For the rationales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_explainers = MyExplainer(label_names, model)\n",
    "\n",
    "my_evaluators = MyEvaluation(label_names, model.my_predict, False, True)\n",
    "my_evaluatorsP = MyEvaluation(label_names, model.my_predict, False, False)\n",
    "evaluation = {'F': my_evaluators.faithfulness, 'FTP': my_evaluators.faithful_truthfulness_penalty,\n",
    "              'NZW': my_evaluators.nzw, 'AUPRC': my_evaluators.auprc}\n",
    "evaluationP = {'F': my_evaluatorsP.faithfulness, 'FTP': my_evaluatorsP.faithful_truthfulness_penalty,\n",
    "               'NZW': my_evaluatorsP.nzw, 'AUPRC': my_evaluators.auprc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6d4c7",
   "metadata": {},
   "source": [
    "We start the experiment measuring the performance of LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62535bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    file_name = save_path + 'HX_DISTILBERT_LIME_IG_' + \\\n",
    "        str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'F': [], 'FTP': [], 'NZW': [], 'AUPRC': []}\n",
    "    metricsP = {'F': [], 'FTP': [], 'NZW': [], 'AUPRC': []}\n",
    "    time_r = [[], []]\n",
    "    my_explainers.neighbours = 2000\n",
    "    techniques = [my_explainers.lime, my_explainers.ig]\n",
    "    for ind in tqdm(range(0, len(test_texts))):\n",
    "        torch.cuda.empty_cache()\n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        prediction, _, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance, instance],\n",
    "                              truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "\n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for technique in techniques:\n",
    "            ts = time.time()\n",
    "            temp = technique(instance, prediction, tokens, mask, _, _)\n",
    "            interpretations.append(\n",
    "                [np.array(i)/np.max(abs(np.array(i))) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            for interpretation in interpretations:\n",
    "                evaluated.append(evaluation[metric](\n",
    "                    interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        my_evaluators.clear_states()\n",
    "        for metric in metrics.keys():\n",
    "            evaluatedP = []\n",
    "            for interpretation in interpretations:\n",
    "                evaluatedP.append(evaluationP[metric](\n",
    "                    interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "            metricsP[metric].append(evaluatedP)\n",
    "        with open(file_name+'(A).pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'(P).pickle', 'wb') as handle:\n",
    "            pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb172e",
   "metadata": {},
   "source": [
    "We present the results for LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+'(A)', [' LIME', ' IG  '], metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+'(P)', [' LIME', ' IG  '], metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da536a7",
   "metadata": {},
   "source": [
    "Then, we perform the experiments for the different attention setups!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4584e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean', 'Multi'] + list(range(6)):\n",
    "    for ce in ['Mean', 'Multi'] + list(range(12)):\n",
    "        # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']:\n",
    "            for cl in [False]:  # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    file_name = save_path + 'HX_DISTILBERT_ATTENTION_' + \\\n",
    "        str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP': [], 'F': [], 'NZW': [], 'AUPRC': []}\n",
    "    metricsP = {'FTP': [], 'F': [], 'NZW': [], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache()\n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, attention, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance, instance],\n",
    "                              truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "\n",
    "        interpretations = []\n",
    "        kk = 0\n",
    "        for con in conf:\n",
    "            ts = time.time()\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(\n",
    "                instance, prediction, tokens, mask, attention, _)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluation[metric](\n",
    "                    interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b.append(k)\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluationP[metric](\n",
    "                    interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b2.append(k)\n",
    "            metricsP[metric].append(evaluated)\n",
    "        # load metrics\n",
    "        if(ind != 0):\n",
    "            with open(file_name+' (A).pickle', 'rb') as handle:\n",
    "                old_metrics = pickle.load(handle)\n",
    "            with open(file_name+' (P).pickle', 'rb') as handle:\n",
    "                old_metricsP = pickle.load(handle)\n",
    "            # append new results\n",
    "            for key in metrics.keys():\n",
    "                old_metrics[key].append(metrics[key][0])\n",
    "                old_metricsP[key].append(metricsP[key][0])\n",
    "        else:\n",
    "            old_metrics = metrics\n",
    "            old_metricsP = metricsP\n",
    "        # save metrics as below\n",
    "\n",
    "        with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "            pickle.dump(old_metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "            pickle.dump(old_metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        del old_metrics, old_metricsP\n",
    "        metrics = {'FTP': [], 'F': [], 'NZW': [], 'AUPRC': []}\n",
    "        metricsP = {'FTP': [], 'F': [], 'NZW': [], 'AUPRC': []}\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(), time_r.mean(axis=1).max(), time_r.mean(\n",
    "    axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a825941",
   "metadata": {},
   "source": [
    "We present the results of the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a55083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ec3f6",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation script at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93200799",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359f74d",
   "metadata": {},
   "source": [
    "We repeat the process with Attention Scores with negative values (A*), thus by skipping the Softmax function. In the attention setups, we exclude the multiplication option in heads and layers, as a few combinations reach +/-inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean'] + list(range(6)):\n",
    "    for ce in ['Mean'] + list(range(12)):\n",
    "        # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']:\n",
    "            for cl in [False]:  # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ceb7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    file_name = save_path + 'HX_DISTILBERT_A_ATTENTION_NO_SOFTMAX_' + \\\n",
    "        str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP': [], 'F': [], 'NZW': [], 'AUPRC': []}\n",
    "    metricsP = {'FTP': [], 'F': [], 'NZW': [], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache()\n",
    "        test_rational = test_test_rationales[ind]\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, _, hidden_states = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance, instance],\n",
    "                              truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "\n",
    "        attention = []\n",
    "        for la in range(6):\n",
    "            our_new_layer = []\n",
    "            bob = model.trainer.model.base_model.transformer.layer[la].attention\n",
    "            has = hidden_states[la]\n",
    "            aaa = bob.k_lin(torch.tensor(has).to('cuda'))\n",
    "            bbb = bob.q_lin(torch.tensor(has).to('cuda'))\n",
    "            for he in range(12):\n",
    "                bbb = bbb / math.sqrt(64)\n",
    "                attention_scores = torch.matmul(\n",
    "                    bbb[:, he*64:(he+1)*64], aaa[:, he*64:(he+1)*64].transpose(-1, -2))\n",
    "                our_new_layer.append(attention_scores.cpu().detach().numpy())\n",
    "            attention.append(our_new_layer)\n",
    "        attention = np.array(attention)\n",
    "        interpretations = []\n",
    "\n",
    "        kk = 0\n",
    "        for con in conf:\n",
    "            ts = time.time()\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(\n",
    "                instance, prediction, tokens, mask, attention, _)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "            time_r[kk].append(time.time()-ts)\n",
    "            kk = kk + 1\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluation[metric](\n",
    "                    interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b.append(k)\n",
    "            metrics[metric].append(evaluated)\n",
    "        my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                tt = time.time()\n",
    "                evaluated.append(evaluationP[metric](\n",
    "                    interpretation, _, instance, prediction, tokens, _, _, test_rational))\n",
    "                k = k + (time.time()-tt)\n",
    "            if metric == 'FTP':\n",
    "                time_b2.append(k)\n",
    "            metricsP[metric].append(evaluated)\n",
    "\n",
    "        if(ind != 0):\n",
    "            with open(file_name+' (A).pickle', 'rb') as handle:\n",
    "                old_metrics = pickle.load(handle)\n",
    "            with open(file_name+' (P).pickle', 'rb') as handle:\n",
    "                old_metricsP = pickle.load(handle)\n",
    "            # append new results\n",
    "            for key in metrics.keys():\n",
    "                old_metrics[key].append(metrics[key][0])\n",
    "                old_metricsP[key].append(metricsP[key][0])\n",
    "        else:\n",
    "            old_metrics = metrics\n",
    "            old_metricsP = metricsP\n",
    "        # save metrics as below\n",
    "\n",
    "        with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "            pickle.dump(old_metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "            pickle.dump(old_metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "            pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        del old_metrics, old_metricsP\n",
    "        metrics = {'FTP': [], 'F': [], 'NZW': [], 'AUPRC': []}\n",
    "        metricsP = {'FTP': [], 'F': [], 'NZW': [], 'AUPRC': []}\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(), time_r.mean(axis=1).max(), time_r.mean(\n",
    "    axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c10c",
   "metadata": {},
   "source": [
    "We present the results for the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744a99f",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation script at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a817b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4712f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240fb0f7",
   "metadata": {},
   "source": [
    "## Qualitative Experiments\n",
    "In this part of the notebook, we present the qualitative experiments, and we use the ready-to-use tool Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdde870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import Optimus, plot_text_heatmap\n",
    "ionbot = Optimus(model, tokenizer, label_names, task, set_of_instance=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0836af4f",
   "metadata": {},
   "source": [
    "We select a random instance and we make prediction for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = test_texts[504]\n",
    "test_rational = test_test_rationales[504]\n",
    "prediction, attention, hidden_states = model.my_predict(instance)\n",
    "enc = model.tokenizer([instance, instance], truncation=True, padding=True)[0]\n",
    "mask = enc.attention_mask\n",
    "tokens = enc.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676db5f",
   "metadata": {},
   "source": [
    "Then, we compute and present interpretations for the different techniques including Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 504\n",
    "plot_text_heatmap(\n",
    "    tokens[1:-1],\n",
    "    np.array(test_test_rationales[ind][1]) #Rationals: Ground Truth\n",
    ")\n",
    "explanation = ionbot.explain( \n",
    "    test_texts[504], mode='baseline', level='token', raw_attention='A')\n",
    "plot_text_heatmap(\n",
    "    tokens[1:-1],\n",
    "    np.array(explanation[0][1]) #Baseline\n",
    ")\n",
    "explanation = ionbot.explain(\n",
    "    test_texts[504], mode='max_per_instance_per_label', level='token', raw_attention='A')\n",
    "plot_text_heatmap(\n",
    "    tokens[1:-1],\n",
    "    np.array(explanation[0][1]) #Optumus Label\n",
    ")\n",
    "\n",
    "temp = my_explainers.ig(instance, prediction, tokens,\n",
    "                        mask, attention, hidden_states)\n",
    "interpretation = [maxabs_scale(i) for i in temp][1]\n",
    "plot_text_heatmap(\n",
    "    tokens[1:-1],\n",
    "    np.array(interpretation) #Integrated Gradients\n",
    ")\n",
    "\n",
    "temp = my_explainers.lime(instance, prediction, tokens,\n",
    "                          mask, attention, hidden_states)\n",
    "interpretation = [maxabs_scale(i) for i in temp][1]\n",
    "plot_text_heatmap(\n",
    "    tokens[1:-1],\n",
    "    np.array(interpretation) #LIME\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
