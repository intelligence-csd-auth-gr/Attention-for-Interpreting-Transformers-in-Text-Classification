{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmgzRgv7FDAc"
   },
   "source": [
    "## HoC DistilBERT Sentence\n",
    "In this notebook we examine the performance of interpretability techniques in the HoC dataset using DistilBERT on sentence level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yv_rg-MbRf3T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, average_precision_score\n",
    "from dataset import Dataset\n",
    "from myModel import MyModel, MyDataset\n",
    "from myExplainers import MyExplainer\n",
    "from myEvaluation import MyEvaluation\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import csv\n",
    "import warnings\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from scipy.special import softmax\n",
    "from helper import print_results, print_results_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model and dataset, defining transformer model, and if rationales are available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yDsc8BC8kb1l"
   },
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "model_path = 'Trained Models/'\n",
    "save_path = '/home/myloniko/ethos/Results/HoC/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XETwLsifUgPm"
   },
   "outputs": [],
   "source": [
    "model_name = 'distilbert'\n",
    "existing_rationales = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MyModel, and the subsequent tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y-UdOoV0iqLY"
   },
   "outputs": [],
   "source": [
    "task = 'multi_label'\n",
    "sentence_level = True\n",
    "labels = 10\n",
    "\n",
    "model = MyModel(model_path, 'distilbert_hoc', model_name, task, labels, False)\n",
    "max_sequence_len = model.tokenizer.max_len_single_sentence\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "G1XX0-OzSBxX"
   },
   "outputs": [],
   "source": [
    "hoc = Dataset(path = data_path)\n",
    "x, y, label_names, rationales = hoc.load_hoc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dataset to train/val/test sets (70/10/20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f--OhPYE7zbN"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "indices = np.arange(len(y))\n",
    "train_texts, test_texts, train_labels, test_labels, _, test_indexes = train_test_split(x, y, indices, test_size=.2, random_state=42)\n",
    "if existing_rationales:\n",
    "    test_rationales = [rationales[x] for x in test_indexes]\n",
    "\n",
    "size = (0.1 * len(y)) / len(train_labels)\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(list(train_texts), train_labels, test_size=size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the rationales for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_rationales = []\n",
    "for test_rational in test_rationales:\n",
    "    label_rationales = []\n",
    "    for label in range(labels):\n",
    "        label_rationales.append([])\n",
    "    for sentence in test_rational:\n",
    "        for label in range(labels):\n",
    "            if label_names[label] in sentence:\n",
    "                label_rationales[label].append(1)\n",
    "            else:\n",
    "                label_rationales[label].append(0)\n",
    "    test_label_rationales.append(label_rationales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we measure the performance of the model using average precision score and f1 score (both macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ysGAaouGWle",
    "outputId": "2251c4d1-cd4a-46fb-b6f1-d91b62d8509a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='753721' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 17:30:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "predictions = []\n",
    "for test_text in test_texts:\n",
    "    outputs = model.my_predict(test_text)\n",
    "    predictions.append(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = tf.constant(predictions, dtype = tf.float32)\n",
    "b = tf.keras.activations.sigmoid(a)\n",
    "predictions = b.numpy()\n",
    "\n",
    "#Multi\n",
    "pred_labels = []\n",
    "for prediction in predictions:\n",
    "    pred_labels.append([1 if i >= 0.5 else 0 for i in prediction])\n",
    "\n",
    "def average_precision_wrapper(y, y_pred, view):\n",
    "    return average_precision_score(y, y_pred.toarray(), average=view)\n",
    "\n",
    "print(average_precision_score(test_labels, pred_labels, average='macro'), f1_score(test_labels, pred_labels, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the explainers and the evaluation module, as well as we define the metrics we want to use. In this case, we use F=Faithfulness, FTP=RFT (Ranked Faithful Truthfulness), NZW=Complexity, AUPRC=For the rationales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "id": "yonjszkYGIP-"
   },
   "outputs": [],
   "source": [
    "my_explainers = MyExplainer(label_names, model, True, 'â€¡')\n",
    "\n",
    "my_evaluators = MyEvaluation(label_names, model.my_predict, True, True)\n",
    "my_evaluatorsP = MyEvaluation(label_names, model.my_predict, True, False)\n",
    "evaluation =  {'F':my_evaluators.faithfulness, 'FTP': my_evaluators.faithful_truthfulness_penalty, \n",
    "          'NZW': my_evaluators.nzw, 'AUPRC': my_evaluators.auprc}\n",
    "evaluationP = {'F':my_evaluatorsP.faithfulness, 'FTP': my_evaluatorsP.faithful_truthfulness_penalty, \n",
    "          'NZW': my_evaluatorsP.nzw, 'AUPRC': my_evaluators.auprc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABJJplyZNObW"
   },
   "source": [
    "We start the experiment measuring the performance of LIME and IG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "AURmLV0aHgcm",
    "outputId": "4f87a2b6-6c8a-4c24-a0e6-b59ce86ce07d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    file_name = save_path + 'HOC_DISTILBERT_SENTENCE_LIME_IG_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'F':[], 'FTP':[], 'NZW':[], 'AUPRC':[]}\n",
    "    metricsP = {'F':[], 'FTP':[], 'NZW':[], 'AUPRC':[]}\n",
    "    time_r = [[],[]]\n",
    "    my_explainers.neighbours = 200\n",
    "    techniques = [my_explainers.lime, my_explainers.ig] \n",
    "    for ind in tqdm(range(0,len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_label_rational = test_label_rationales[ind].copy()\n",
    "        instance = test_texts[ind]\n",
    "        if len(instance.split('.')) -1 < len(test_label_rational[0]):\n",
    "            for label in range(labels):\n",
    "                test_label_rational[label] = test_label_rational[label][:len(instance.split('.'))-1]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        prediction, _, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        if tokens.count('.') >= 2:\n",
    "            interpretations = []\n",
    "            kk = 0\n",
    "            for technique in techniques:\n",
    "                ts = time.time()\n",
    "                temp = technique(instance, prediction, tokens, mask, _, _)\n",
    "                temp_tokens = tokens.copy()\n",
    "                if sentence_level:\n",
    "                    temp_tokens = temp[0].copy()[0]\n",
    "                    temp = temp[1].copy()\n",
    "                interpretations.append([np.array(i)/np.max(abs(np.array(i))) for i in temp])\n",
    "                time_r[kk].append(time.time()-ts)\n",
    "                kk = kk + 1\n",
    "            for metric in metrics.keys():\n",
    "                evaluated = []\n",
    "                for interpretation in interpretations:\n",
    "                    evaluated.append(evaluation[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_label_rational))\n",
    "                metrics[metric].append(evaluated)\n",
    "            my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "            my_evaluators.clear_states()\n",
    "            for metric in metrics.keys():\n",
    "                evaluatedP = []\n",
    "                for interpretation in interpretations:\n",
    "                    evaluatedP.append(evaluationP[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_label_rational))\n",
    "                metricsP[metric].append(evaluatedP)\n",
    "            with open(file_name+'(A).pickle', 'wb') as handle:\n",
    "                pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(file_name+'(P).pickle', 'wb') as handle:\n",
    "                pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "                pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the results for LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YULHsaDbOcyr",
    "outputId": "06976d5f-d158-4880-ae07-3fa03bccaa5d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results(file_name+'(A)', [' LIME', ' IG  '], metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+'(P)', [' LIME', ' IG  '], metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vopsJXDmFsh-"
   },
   "source": [
    "Then, we perform the experiments for the different attention setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGXi71XEIxuU",
    "outputId": "63f332e0-d823-4da2-b928-4d322576e6d7"
   },
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean', 'Multi'] + list(range(6)):\n",
    "    for ce in ['Mean'] + list(range(12)):\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'HoC_DISTILBERT_ATTENTION_SENTENCE'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    metricsP = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_label_rational = test_label_rationales[ind].copy()\n",
    "        instance = test_texts[ind]\n",
    "        if len(instance.split('.')) -1 < len(test_label_rational[0]):\n",
    "            for label in range(labels):\n",
    "                test_label_rational[label] = test_label_rational[label][:len(instance.split('.'))-1]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, attention, _ = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        \n",
    "        if tokens.count('.') >= 2:\n",
    "            interpretations = []\n",
    "            kk = 0\n",
    "            for con in conf:\n",
    "                ts = time.time()\n",
    "                my_explainers.config = con\n",
    "                temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, _)\n",
    "                temp_tokens = tokens.copy()\n",
    "                if sentence_level:\n",
    "                    temp_tokens = temp[0].copy()[0]\n",
    "                    temp = temp[1].copy()\n",
    "                interpretations.append([np.array(i)/np.max(abs(np.array(i))) for i in temp])\n",
    "                time_r[kk].append(time.time()-ts)\n",
    "                kk = kk + 1\n",
    "            for metric in metrics.keys():\n",
    "                evaluated = []\n",
    "                k = 0\n",
    "                for interpretation in interpretations:\n",
    "                    tt = time.time()\n",
    "                    evaluated.append(evaluation[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_label_rational))\n",
    "                    k = k + (time.time()-tt)\n",
    "                if metric == 'FTP':\n",
    "                    time_b.append(k)\n",
    "                metrics[metric].append(evaluated)\n",
    "            my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "            for metric in metrics.keys():\n",
    "                evaluated = []\n",
    "                k = 0\n",
    "                for interpretation in interpretations:\n",
    "                    tt = time.time()\n",
    "                    evaluated.append(evaluationP[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_label_rational))\n",
    "                    k = k + (time.time()-tt)\n",
    "                if metric == 'FTP':\n",
    "                    time_b2.append(k)\n",
    "                metricsP[metric].append(evaluated)\n",
    "            with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "                pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "                pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "                pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(),time_r.mean(axis=1).max(), time_r.mean(axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the results of the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the process with Attention Scores with negative values (A*), thus by skipping the Softmax function. In the attention setups, we exclude the multiplication option in heads and layers, as a few combinations reach +/-inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = []\n",
    "for ci in ['Mean'] + list(range(6)):\n",
    "    for ce in ['Mean'] + list(range(12)):\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MaxColumns']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'HoC_DISTILBERT_SENTENCE_ATTENTION_NO_SOFTMAX_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    metricsP = {'FTP':[], 'F':[], 'NZW':[], 'AUPRC': []}\n",
    "    time_r = []\n",
    "    time_b = []\n",
    "    time_b2 = []\n",
    "    for con in conf:\n",
    "        time_r.append([])\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache() \n",
    "        test_label_rational = test_label_rationales[ind].copy()\n",
    "        instance = test_texts[ind]\n",
    "        if len(instance.split('.')) -1 < len(test_label_rational[0]):\n",
    "            for label in range(labels):\n",
    "                test_label_rational[label] = test_label_rational[label][:len(instance.split('.'))-1]\n",
    "        my_evaluators.clear_states()\n",
    "        my_evaluatorsP.clear_states()\n",
    "        my_explainers.save_states = {}\n",
    "        prediction, _, hidden_states = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        \n",
    "        if tokens.count('.') >= 2:\n",
    "            attention = []\n",
    "            for la in range(6):\n",
    "                our_new_layer = []\n",
    "                bob =  model.trainer.model.base_model.transformer.layer[la].attention\n",
    "                has = hidden_states[la]\n",
    "                aaa = bob.k_lin(torch.tensor(has).to('cuda'))\n",
    "                bbb = bob.q_lin(torch.tensor(has).to('cuda'))\n",
    "                for he in range(12):\n",
    "                    bbb = bbb / math.sqrt(64)\n",
    "                    attention_scores = torch.matmul(bbb[:,he*64:(he+1)*64], aaa[:,he*64:(he+1)*64].transpose(-1, -2))\n",
    "                    our_new_layer.append(attention_scores.cpu().detach().numpy())\n",
    "                attention.append(our_new_layer)\n",
    "            attention = np.array(attention)\n",
    "            interpretations = []\n",
    "            kk = 0\n",
    "            for con in conf:\n",
    "                ts = time.time()\n",
    "                my_explainers.config = con\n",
    "                temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, _)\n",
    "                temp_tokens = tokens.copy()\n",
    "                if sentence_level:\n",
    "                    temp_tokens = temp[0].copy()[0]\n",
    "                    temp = temp[1].copy()\n",
    "                interpretations.append([np.array(i)/np.max(abs(np.array(i))) for i in temp])\n",
    "                time_r[kk].append(time.time()-ts)\n",
    "                kk = kk + 1\n",
    "            for metric in metrics.keys():\n",
    "                evaluated = []\n",
    "                k = 0\n",
    "                for interpretation in interpretations:\n",
    "                    tt = time.time()\n",
    "                    evaluated.append(evaluation[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_label_rational))\n",
    "                    k = k + (time.time()-tt)\n",
    "                if metric == 'FTP':\n",
    "                    time_b.append(k)\n",
    "                metrics[metric].append(evaluated)\n",
    "            my_evaluatorsP.saved_state = my_evaluators.saved_state.copy()\n",
    "            for metric in metrics.keys():\n",
    "                evaluated = []\n",
    "                k = 0\n",
    "                for interpretation in interpretations:\n",
    "                    tt = time.time()\n",
    "                    evaluated.append(evaluationP[metric](interpretation, _, instance, prediction, temp_tokens, _, _, test_label_rational))\n",
    "                    k = k + (time.time()-tt)\n",
    "                if metric == 'FTP':\n",
    "                    time_b2.append(k)\n",
    "                metricsP[metric].append(evaluated)        \n",
    "            with open(file_name+' (A).pickle', 'wb') as handle:\n",
    "                pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(file_name+' (P).pickle', 'wb') as handle:\n",
    "                pickle.dump(metricsP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(file_name+'_TIME.pickle', 'wb') as handle:\n",
    "                pickle.dump(time_r, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_r = np.array(time_r)\n",
    "time_r.mean(axis=1).min(),time_r.mean(axis=1).max(), time_r.mean(axis=1).mean(), time_r.sum(axis=1).mean(), np.mean(time_b), np.mean(time_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the results for the different attention setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (A)', conf, metrics, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(file_name+' (P)', conf, metricsP, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the best attention setup using Optimus variations (we do not use the Optimus implementation script at this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results_ap(metrics, label_names, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_ap(metricsP, label_names, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Experiments\n",
    "In this part of the notebook, we present the qualitative experiments, and we use the ready-to-use tool Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import Optimus, plot_sentence_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ionbot = Optimus(model, tokenizer, label_names, task, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a random instance and we make prediction for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "inddd = 211\n",
    "instance = test_texts[inddd]\n",
    "prediction, attention, hidden_states = model.my_predict(instance)\n",
    "enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "mask = enc.attention_mask\n",
    "tokens = enc.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the baseline interpretation as well as the one from Optimus_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ionbot.explain(instance, mode='baseline', level='sentence', raw_attention='A')\n",
    "explanation = ionbot.explain(instance, mode='max_per_instance_per_label', level='sentence', raw_attention='A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the interpretation from IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = my_explainers.ig(instance, prediction, tokens, mask, _, _)\n",
    "temp_tokens = tokens.copy()\n",
    "if sentence_level:\n",
    "    temp_tokens = ig[0].copy()[0]\n",
    "    ig = ig[1].copy()\n",
    "ig = [np.array(i)/np.max(abs(np.array(i))) for i in ig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the interpretation from LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_explainers.neighbours = 200\n",
    "lime = my_explainers.lime(instance, prediction, tokens, mask, _, _)\n",
    "temp_tokens = tokens.copy()\n",
    "if sentence_level:\n",
    "    temp_tokens = lime[0].copy()[0]\n",
    "    lime = lime[1].copy()\n",
    "lime = [np.array(i)/np.max(abs(np.array(i))) for i in lime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the feature importance score from each sentence to each word comprising it for the different techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 7\n",
    "n_tokens = []\n",
    "rationalee = []\n",
    "baseline_weights = []\n",
    "weights = []\n",
    "ig_weights = []\n",
    "lime_weights = []\n",
    "c = 0\n",
    "for sentence in explanation[1]:\n",
    "    for word in sentence.split(' '):\n",
    "        n_tokens.append(word)\n",
    "        baseline_weights.append(baseline[0][label][c])\n",
    "        weights.append(explanation[0][label][c])\n",
    "        ig_weights.append(ig[label][c])\n",
    "        lime_weights.append(lime[label][c])\n",
    "        rationalee.append(test_label_rationales[inddd][label][c])\n",
    "    c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the feature importance weights getting our interpretation from each technique as well as the ground truth rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentence_heatmap(\n",
    "    n_tokens,\n",
    "    np.array(rationalee)\n",
    ")\n",
    "plot_sentence_heatmap(\n",
    "    n_tokens,\n",
    "    np.array(baseline_weights)\n",
    ")\n",
    "plot_sentence_heatmap(\n",
    "    n_tokens,\n",
    "    np.array(weights)\n",
    ")\n",
    "plot_sentence_heatmap(\n",
    "    n_tokens,\n",
    "    np.array(ig_weights)\n",
    ")\n",
    "plot_sentence_heatmap(\n",
    "    n_tokens,\n",
    "    np.array(lime_weights)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QnSfmbatE-0U",
    "UmgzRgv7FDAc",
    "ABJJplyZNObW",
    "vopsJXDmFsh-"
   ],
   "name": "2. AtMa Ethos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
