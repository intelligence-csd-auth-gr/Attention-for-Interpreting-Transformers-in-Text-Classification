{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmgzRgv7FDAc"
   },
   "source": [
    "## Test Interpretability techniques in HoC with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yv_rg-MbRf3T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from dataset import Dataset\n",
    "from myModel import MyModel\n",
    "from myExplainers import MyExplainer\n",
    "from myEvaluation import MyEvaluation\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import csv\n",
    "import warnings\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yDsc8BC8kb1l"
   },
   "outputs": [],
   "source": [
    "data_path = '/models/'\n",
    "model_path = '/models/'\n",
    "save_path = '/results/HoC/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XETwLsifUgPm"
   },
   "outputs": [],
   "source": [
    "model_name = 'bert'\n",
    "existing_rationales = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y-UdOoV0iqLY"
   },
   "outputs": [],
   "source": [
    "task = 'multi_label'\n",
    "sentence_level = False\n",
    "labels = 10\n",
    "\n",
    "model = MyModel(model_path, 'bert_hoc2', model_name, task, labels, False)\n",
    "max_sequence_len = model.tokenizer.max_len_single_sentence\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G1XX0-OzSBxX"
   },
   "outputs": [],
   "source": [
    "hoc = Dataset(path = data_path)\n",
    "x, y, label_names, rationales = hoc.load_hoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f--OhPYE7zbN"
   },
   "outputs": [],
   "source": [
    "indices = np.arange(len(y))\n",
    "train_texts, test_texts, train_labels, test_labels, _, test_indexes = train_test_split(x, y, indices, test_size=.2, random_state=42)\n",
    "if existing_rationales:\n",
    "    test_rationales = [rationales[x] for x in test_indexes]\n",
    "\n",
    "size = (0.1 * len(y)) / len(train_labels)\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(list(train_texts), train_labels, test_size=size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ysGAaouGWle",
    "outputId": "2251c4d1-cd4a-46fb-b6f1-d91b62d8509a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52032' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 1:37:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = []\n",
    "for test_text in test_texts:\n",
    "    outputs = model.my_predict(test_text)\n",
    "    predictions.append(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(predictions, dtype = tf.float32)\n",
    "b = tf.keras.activations.sigmoid(a)\n",
    "predictions = b.numpy()\n",
    "\n",
    "#Multi\n",
    "pred_labels = []\n",
    "for prediction in predictions:\n",
    "    pred_labels.append([1 if i >= 0.5 else 0 for i in prediction])\n",
    "\n",
    "def average_precision_wrapper(y, y_pred, view):\n",
    "    return average_precision_score(y, y_pred.toarray(), average=view)\n",
    "\n",
    "print(average_precision_score(test_labels, pred_labels, average='macro'), f1_score(test_labels, pred_labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yonjszkYGIP-"
   },
   "outputs": [],
   "source": [
    "my_explainers = MyExplainer(label_names, model)\n",
    "my_evaluators = MyEvaluation(label_names, model.my_predict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Cj8OHywtNbvy"
   },
   "outputs": [],
   "source": [
    "def print_results(name, techniques, metrics):\n",
    "\twith open(name+'.csv', 'w', encoding='UTF8') as f:  \n",
    "\t\twriter = csv.writer(f)\n",
    "\t\tfor metric in metrics.keys():\n",
    "\t\t\tprint(metric)\n",
    "\t\t\ttemp_metric = np.array(metrics[metric])\n",
    "\t\t\tfor i in range(len(techniques)):\n",
    "\t\t\t\tlabel_score = []\n",
    "\t\t\t\tfor label in range(len(label_names)):\n",
    "\t\t\t\t\ttempo = [k for k in temp_metric[:,i,label] if str(k) != str(np.average([])) ]\n",
    "\t\t\t\t\tif len(tempo) == 0:\n",
    "\t\t\t\t\t\t\ttempo.append(0)\n",
    "\t\t\t\t\tlabel_score.append(np.array(tempo))\n",
    "\t\t\t\ttemp_mean = []\n",
    "\t\t\t\tfor k in label_score:\n",
    "\t\t\t\t\ttemp_mean.append(k.mean())\n",
    "\t\t\t\ttemp_mean = np.array(temp_mean).mean()\n",
    "\t\t\t\twriter.writerow([techniques[i],metric,temp_mean]+[label_score[o].mean() for o in range(len(label_names))])\n",
    "\t\t\t\tprint(techniques[i],' {} | {}'.format(round(temp_mean,5),' '.join([str(round(label_score[o].mean(),5)) for o in range(len(label_names))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABJJplyZNObW"
   },
   "source": [
    "## Evaluation of LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "AURmLV0aHgcm",
    "outputId": "4f87a2b6-6c8a-4c24-a0e6-b59ce86ce07d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'hoc_TOKEN_LIME_IG_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'F':[], 'FTP':[], 'NZW':[]}\n",
    "    evaluation = {'F':my_evaluators.faithfulness, 'FTP': my_evaluators.faithful_truthfulness_penalty, 'NZW': my_evaluators.nzw}\n",
    "\n",
    "    techniques = [my_explainers.lime, my_explainers.ig] \n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache()\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        prediction, attention, hidden_states = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        interpretations = []\n",
    "        for technique in techniques:\n",
    "            temp = technique(instance, prediction, tokens, mask, attention, hidden_states)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, tokens, hidden_states, _, _))\n",
    "                k = k + 1\n",
    "            metrics[metric].append(evaluated)\n",
    "\n",
    "        with open(file_name+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YULHsaDbOcyr",
    "outputId": "06976d5f-d158-4880-ae07-3fa03bccaa5d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results(file_name, [' LIME', ' IG  '], metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vopsJXDmFsh-"
   },
   "source": [
    "## Evaluation of Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGXi71XEIxuU",
    "outputId": "63f332e0-d823-4da2-b928-4d322576e6d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = []\n",
    "for ci in ['Mean', 'Multi', 'Sum'] + list(range(12)): # Layers: Mean, Multi, Sum, First, Last\n",
    "    for ce in ['Mean', 'Sum'] + list(range(12)): #True every token, False only cls\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MeanRows', 'MaxColumns', 'MaxRows']: # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]: # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "for ci in ['Mean', 'Multi', 'Sum']: \n",
    "    for ce in ['']:\n",
    "        for cp in ['']: \n",
    "            for cl in [True]: \n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "3tMJuavo-jZZ",
    "outputId": "75a65185-9d90-4dc8-bf40-10707d42dd72",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    file_name = save_path + 'hoc_TOKEN_Attention_'+str(now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'F':[], 'FTP':[], 'NZW':[]}\n",
    "    evaluation = {'F':my_evaluators.faithfulness, 'FTP': my_evaluators.faithful_truthfulness_penalty, \n",
    "                  'NZW': my_evaluators.nzw}\n",
    "\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache()\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        prediction, attention, hidden_states = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance,instance], truncation=True, padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        interpretations = []\n",
    "        for con in conf:   \n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(instance, prediction, tokens, mask, attention, hidden_states)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                evaluated.append(evaluation[metric](interpretation, _, instance, prediction, tokens, hidden_states, _, _))\n",
    "                k = k + 1\n",
    "            metrics[metric].append(evaluated)\n",
    "\n",
    "        with open(file_name+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CubZh0w_CUp"
   },
   "outputs": [],
   "source": [
    "print_results(file_name, conf, metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QnSfmbatE-0U",
    "UmgzRgv7FDAc",
    "ABJJplyZNObW",
    "vopsJXDmFsh-"
   ],
   "name": "2. AtMa Ethos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
