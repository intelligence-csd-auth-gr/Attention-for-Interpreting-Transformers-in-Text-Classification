{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmgzRgv7FDAc"
   },
   "source": [
    "## Test Interpretability techniques in AIS with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yv_rg-MbRf3T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from dataset import Dataset\n",
    "from myModel import MyModel\n",
    "from myExplainers import MyExplainer\n",
    "from myEvaluation import MyEvaluation\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import csv\n",
    "import torch\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yDsc8BC8kb1l"
   },
   "outputs": [],
   "source": [
    "data_path = '/models/'\n",
    "model_path = '/models/'\n",
    "save_path = '/results/AIS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XETwLsifUgPm"
   },
   "outputs": [],
   "source": [
    "model_name = 'bert'\n",
    "existing_rationales = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y-UdOoV0iqLY"
   },
   "outputs": [],
   "source": [
    "task = 'single_label'\n",
    "sentence_level = False\n",
    "labels = 2\n",
    "\n",
    "model = MyModel(model_path, 'bert_ais', model_name, task, labels, 'cased')\n",
    "max_sequence_len = model.tokenizer.max_len_single_sentence\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G1XX0-OzSBxX"
   },
   "outputs": [],
   "source": [
    "ais = Dataset(path=data_path)\n",
    "x, y, label_names = ais.load_AIS()\n",
    "label_names = ['class a', 'class b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f--OhPYE7zbN"
   },
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    x, y, test_size=.2, random_state=42)\n",
    "\n",
    "size = (0.1 * len(y)) / len(train_labels)\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(\n",
    "    list(train_texts), train_labels, test_size=size, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ysGAaouGWle",
    "outputId": "2251c4d1-cd4a-46fb-b6f1-d91b62d8509a"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for test_text in test_texts:\n",
    "    outputs = model.my_predict(test_text)\n",
    "    predictions.append(outputs[0])\n",
    "\n",
    "#Binary\n",
    "pred_labels = []\n",
    "for prediction in predictions:\n",
    "    pred_labels.append(np.argmax(softmax(prediction)))\n",
    "\n",
    "\n",
    "def average_precision_wrapper(y, y_pred, view):\n",
    "    return average_precision_score(y, y_pred.toarray(), average=view)\n",
    "\n",
    "\n",
    "print(test_labels[:3])\n",
    "print(pred_labels[:3])\n",
    "\n",
    "print(average_precision_score(test_labels, pred_labels, average='macro'),\n",
    "      f1_score(test_labels, pred_labels, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yonjszkYGIP-"
   },
   "outputs": [],
   "source": [
    "my_explainers = MyExplainer(label_names, model)\n",
    "my_evaluators = MyEvaluation(label_names, model.my_predict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Cj8OHywtNbvy"
   },
   "outputs": [],
   "source": [
    "def print_results(name, techniques, metrics):\n",
    "    with open(name + '.csv', 'w', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for metric in metrics.keys():\n",
    "            print(metric)\n",
    "            temp_metric = np.array(metrics[metric])\n",
    "            for i in range(len(techniques)):\n",
    "                label_score = []\n",
    "                for label in range(len(label_names)):\n",
    "                    tempo = [\n",
    "                        k for k in temp_metric[:, i, label]\n",
    "                        if str(k) != str(np.average([]))\n",
    "                    ]\n",
    "                    if len(tempo) == 0:\n",
    "                        tempo.append(0)\n",
    "                    label_score.append(np.array(tempo))\n",
    "                temp_mean = []\n",
    "                for k in label_score:\n",
    "                    temp_mean.append(k.mean())\n",
    "                temp_mean = np.array(temp_mean).mean()\n",
    "                writer.writerow(\n",
    "                    [techniques[i], metric, temp_mean] +\n",
    "                    [label_score[o].mean() for o in range(len(label_names))])\n",
    "                print(\n",
    "                    techniques[i], ' {} | {}'.format(\n",
    "                        round(temp_mean, 5), ' '.join([\n",
    "                            str(round(label_score[o].mean(), 5))\n",
    "                            for o in range(len(label_names))\n",
    "                        ])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABJJplyZNObW"
   },
   "source": [
    "## Evaluating LIME and IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "AURmLV0aHgcm",
    "outputId": "4f87a2b6-6c8a-4c24-a0e6-b59ce86ce07d"
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    file_name = save_path + 'BERT_ais_LIME_IG_' + str(now.day) + '_' + str(\n",
    "        now.month) + '_' + str(now.year)\n",
    "    metrics = {'F': [], 'FTP': [], 'R': [], 'NZW': []}\n",
    "    evaluation = {\n",
    "        'F': my_evaluators.faithfulness,\n",
    "        'FTP': my_evaluators.faithful_truthfulness_penalty,\n",
    "        'R': my_evaluators.robustness,\n",
    "        'NZW': my_evaluators.nzw\n",
    "    }\n",
    "\n",
    "    techniques = [my_explainers.lime, my_explainers.ig]\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache()\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        prediction, attention, hidden_states = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance, instance],\n",
    "                              truncation=True,\n",
    "                              padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        interpretations = []\n",
    "        for technique in techniques:\n",
    "            temp = technique(instance, prediction, tokens, mask, attention,\n",
    "                             hidden_states)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                evaluated.append(evaluation[metric](interpretation, _,\n",
    "                                                    instance, prediction,\n",
    "                                                    tokens, hidden_states, _,\n",
    "                                                    _))\n",
    "            k = k + 1\n",
    "            metrics[metric].append(evaluated)\n",
    "\n",
    "        with open(file_name + '.pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YULHsaDbOcyr",
    "outputId": "06976d5f-d158-4880-ae07-3fa03bccaa5d"
   },
   "outputs": [],
   "source": [
    "print_results(file_name, [' LIME', ' IG  '], metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vopsJXDmFsh-"
   },
   "source": [
    "## Evaluation of Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGXi71XEIxuU",
    "outputId": "63f332e0-d823-4da2-b928-4d322576e6d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = []\n",
    "for ci in ['Mean', 'Multi', 'Sum'] + list( range(12)):  # Layers: Mean, Multi, Sum, First, Last\n",
    "    for ce in ['Mean', 'Sum'] + list(range(12)):  #True every token, False only cls\n",
    "        for cp in ['From', 'To', 'MeanColumns', 'MeanRows', 'MaxColumns','MaxRows']:  # Matrix: From, To, MeanColumns, MeanRows, MaxColumns, MaxRows\n",
    "            for cl in [False]:  # Selection: True: select layers per head, False: do not\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "for ci in ['Mean', 'Multi', 'Sum']:\n",
    "    for ce in ['']:\n",
    "        for cp in ['']:\n",
    "            for cl in [True]:\n",
    "                conf.append([ci, ce, cp, cl])\n",
    "len(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "3tMJuavo-jZZ",
    "outputId": "75a65185-9d90-4dc8-bf40-10707d42dd72",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    file_name = save_path + 'BERT_ais_TOKEN_Attention_' + str(\n",
    "        now.day) + '_' + str(now.month) + '_' + str(now.year)\n",
    "    metrics = {'F': [], 'FTP': [], 'NZW': []}\n",
    "    evaluation = {\n",
    "        'F': my_evaluators.faithfulness,\n",
    "        'FTP': my_evaluators.faithful_truthfulness_penalty,\n",
    "        'NZW': my_evaluators.nzw\n",
    "    }\n",
    "\n",
    "    for ind in tqdm(range(len(test_texts))):\n",
    "        torch.cuda.empty_cache()\n",
    "        instance = test_texts[ind]\n",
    "        my_evaluators.clear_states()\n",
    "        prediction, attention, hidden_states = model.my_predict(instance)\n",
    "        enc = model.tokenizer([instance, instance],\n",
    "                              truncation=True,\n",
    "                              padding=True)[0]\n",
    "        mask = enc.attention_mask\n",
    "        tokens = enc.tokens\n",
    "        interpretations = []\n",
    "        for con in conf:\n",
    "            my_explainers.config = con\n",
    "            temp = my_explainers.my_attention(instance, prediction, tokens,\n",
    "                                              mask, attention, hidden_states)\n",
    "            interpretations.append([maxabs_scale(i) for i in temp])\n",
    "        for metric in metrics.keys():\n",
    "            evaluated = []\n",
    "            k = 0\n",
    "            for interpretation in interpretations:\n",
    "                evaluated.append(evaluation[metric](interpretation, _,\n",
    "                                                    instance, prediction,\n",
    "                                                    tokens, hidden_states, _,\n",
    "                                                    _))\n",
    "            k = k + 1\n",
    "            metrics[metric].append(evaluated)\n",
    "\n",
    "        with open(file_name + '.pickle', 'wb') as handle:\n",
    "            pickle.dump(metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CubZh0w_CUp"
   },
   "outputs": [],
   "source": [
    "print_results(file_name, conf, metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QnSfmbatE-0U",
    "UmgzRgv7FDAc",
    "ABJJplyZNObW",
    "vopsJXDmFsh-"
   ],
   "name": "2. AtMa Ethos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
